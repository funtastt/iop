# Поисковая система — Основы информационного поиска (КФУ)

Полнофункциональная поисковая система, разработанная в рамках курса
«Основы информационного поиска» Казанского федерального университета.

**Авторы:**
- Асадуллин Салават Рустамович, группа 11-201, 4 курс
- Лобанов Никита Михайлович, группа 11-201, 4 курс

## Текущая версия: Итерация 1 — Веб-краулер

### Что реализовано

- Скачивание HTML-страниц по заданному списку URL-адресов
- Сохранение страниц **с HTML-разметкой** в директорию `pages/`
- Создание индексного файла `index.txt` (номер → URL)
- Обработка ошибок (таймауты, недоступные страницы)
- Возможность **продолжить скачивание** с места остановки
- Логирование в консоль и в файл `crawler.log`

### Источник данных

Русскоязычные статьи Википедии (120 URL) по темам: информатика,
программирование, математика, наука, география.

---

## Требования

- Python 3.10 или новее
- pip (менеджер пакетов Python)

## Установка и запуск

### 1. Клонировать репозиторий

```bash
git clone <URL_РЕПОЗИТОРИЯ>
cd <ИМЯ_ПАПКИ>
```

### 2. Создать виртуальное окружение (рекомендуется)

```bash
python -m venv .venv
# Windows:
.venv\Scripts\activate
# Linux/macOS:
source .venv/bin/activate
```

### 3. Установить зависимости

```bash
pip install -r requirements.txt
```

### 4. Запустить краулер

```bash
python crawler.py
```

Краулер скачает все страницы из `urls_list.txt` и сохранит их в `pages/`.

---

## Структура проекта

```
project/
├── crawler.py          # Веб-краулер (Итерация 1)
├── requirements.txt    # Зависимости Python
├── urls_list.txt       # Список URL для скачивания
├── README.md           # Документация
├── SUBMISSION_CHECKLIST.md  # Чеклист для сдачи
├── pages/              # Скачанные HTML-страницы
│   ├── page_001.html
│   ├── page_002.html
│   └── ...
├── index.txt           # Индекс: номер файла -> URL
└── crawler.log         # Лог работы краулера
```

## Формат index.txt

```
1 https://ru.wikipedia.org/wiki/Информатика
2 https://ru.wikipedia.org/wiki/Алгоритм
3 https://ru.wikipedia.org/wiki/Программирование
...
```

## Повторный запуск

Краулер автоматически определяет уже скачанные страницы по `index.txt`
и пропускает их. Для полного перезапуска удалите `index.txt` и папку `pages/`.

## Известные ограничения

- Краулер работает последовательно (без параллелизма)
- Скачивает только страницы из заранее подготовленного списка URL
- Задержка 0.5 с между запросами для соблюдения правил сайта

---

## Release Notes

### v1.0 — Итерация 1: Веб-краулер
- Реализовано скачивание HTML-страниц по списку URL
- Сохранение страниц с полной HTML-разметкой
- Создание индексного файла `index.txt`
- Обработка ошибок и логирование
- Возможность продолжить с места остановки
